<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>A Pelican Blog - Other</title><link href="/" rel="alternate"></link><link href="/feeds/other.atom.xml" rel="self"></link><id>/</id><updated>2017-09-16T13:10:00+00:00</updated><entry><title>Network-wide Advertisement Blocking</title><link href="/network-wide-advertisement-blocking.html" rel="alternate"></link><published>2017-09-16T13:10:00+00:00</published><updated>2017-09-16T13:10:00+00:00</updated><author><name></name></author><id>tag:None,2017-09-16:/network-wide-advertisement-blocking.html</id><summary type="html">&lt;p&gt;Recently, I setup my own home wifi network with Comcast.. &lt;em&gt;shudder&lt;/em&gt; However, it initially hasn't been that bad despite their lackluster reputation. I consistently get 75+ Mbps for my plan that costs $35.00 per month, a decent deal for a basic necessity of mine, wireless internet. &lt;/p&gt;
&lt;p&gt;I am a …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Recently, I setup my own home wifi network with Comcast.. &lt;em&gt;shudder&lt;/em&gt; However, it initially hasn't been that bad despite their lackluster reputation. I consistently get 75+ Mbps for my plan that costs $35.00 per month, a decent deal for a basic necessity of mine, wireless internet. &lt;/p&gt;
&lt;p&gt;I am a person who has little patience for online advertising. Luckily, I saw this interesting project online about how you can use a rapsberry pi as a DNS server. A DNS server translates human-memorable domain names and hostnames into their corresponding IP addresses. The pi-hole doesn't sit in between your connected devices and the internet, but rather it is a DNS server that blacklists certain domain names, typically ones that host content. As a result, it doesn't allow any content to be retrieved if that hostname is contained within a
certain database of domain names. So, the router goes to the DNS server and says "hey I'm looking for this domain name, what's the IP address?" &lt;/p&gt;
&lt;p&gt;This allows all of the devices on your wireless network to enjoy advertisement-free internet browsing! Here are the steps that I took to get it set up. &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Download the SD Formatter tool software from SanDisk&lt;/li&gt;
&lt;li&gt;Plug in a 8gb+ microSD card into a computer (you may have to use an adapter to SD depending on your computer) &lt;/li&gt;
&lt;li&gt;Using SD Formatter tool, select and overwrite the SD card to default settings, then quick format the card&lt;/li&gt;
&lt;li&gt;Download the latest version of NOOBS (New Out Of Box Software) from the &lt;a href="https://www.raspberrypi.org/downloads/noobs/"&gt;Raspberry Pi Foundation's website&lt;/a&gt;. Unzip the file. &lt;/li&gt;
&lt;li&gt;Copy the contents of the zip directory to the microSD card that you just formatted. &lt;/li&gt;
&lt;li&gt;Safely eject the SD card adapter&lt;/li&gt;
&lt;li&gt;Plug in the microSD card into the bottom of the raspberry pi, facing up&lt;/li&gt;
&lt;li&gt;Plug the power cord into raspberry pi, hook up to monitor and keyboard&lt;/li&gt;
&lt;li&gt;Select Raspbian OS (Based on Debian), hit the "i" key, wait for the OS to be installed&lt;/li&gt;
&lt;li&gt;Connect to wifi, then open terminal and copy and paste: curl -sSL https://install.pi-hole.net | bash &lt;/li&gt;
&lt;li&gt;Select the appropriate wireless/ethernet connection in setup &lt;/li&gt;
&lt;li&gt;Reserve static IP adress for raspberry pi&lt;/li&gt;
&lt;li&gt;Make sure to modify your router's network settings to use the static IP of the raspberry pi as a static DNS server &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Until next time,&lt;/p&gt;
&lt;h4&gt;Clayton Blythe | &lt;em&gt;Deep Python&lt;/em&gt;&lt;/h4&gt;</content></entry><entry><title>Blogging with Vim and Pelican</title><link href="/blogging-with-vim-and-pelican.html" rel="alternate"></link><published>2017-09-09T13:10:00+00:00</published><updated>2017-09-09T13:10:00+00:00</updated><author><name></name></author><id>tag:None,2017-09-09:/blogging-with-vim-and-pelican.html</id><summary type="html">&lt;p&gt;I have started to learn vim, and it has been a little painful to be honest. However, I have found it to be pretty liberating in that I barely need to use a mouse to do work on my laptop, especially for things like manipulating files, writing code, and even …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I have started to learn vim, and it has been a little painful to be honest. However, I have found it to be pretty liberating in that I barely need to use a mouse to do work on my laptop, especially for things like manipulating files, writing code, and even editing/building my blog. &lt;/p&gt;
&lt;p&gt;I am slowly learning the tricks to quickly and efficiently navigate file structures, make changes to files, and even run normal bash commands as well. It was quit intimidating at first, but I am already finding it rewarding. Now, I have even written some custom commands and scripts to integrate my blog editing and deployment process that uses Pelican, a python utility for building sites and converting markdown text files to HTML. &lt;/p&gt;
&lt;p&gt;I think that it is important to be able to work as quickly and efficiently as possible, and not be hampered by menial things like taking time to open files, make changes, save them, and then see how the changes impact what you are trying to build or accomplish. The goal here is to make sure that my fingers and ability to navigate file structures aren't the limiting factors for my productivity, but rather the speed at which I can think and come up with solutions or content. &lt;/p&gt;
&lt;p&gt;For my blog, my workflow is as follows: I type "serve" in the command line which is a bash alias for changing to the directory of my blog, running a python/pelican development server which serves a local host replica of my blog. The blog is automatically opened in google chrome at http://localhost.com:8000 for me to browse and critique, as this 8000 port is the default for pelican. In an iterm2 terminal, I launch vim with the command "v", hit the leader key "," and shortly after "f" to open a small file browser plugin, which shows at the bottom most recently used files. There I can quickly move to different posts I had been working
on and use standard vim commands. In normal mode I type the leader key ',' and then shortly after I type 'z'. This allows me to perform edits in "focus mode", a minimalistic mode for the vim text editor that I find quite useful and aesthetically pleasing. This focus mode functionality comes from a vim plugin that comes with a vimrc repository that I will link at the bottom.&lt;/p&gt;
&lt;p&gt;One important customization I added to vim was to make the Caps Lock serve as an escape key, so I can easily revert back to normal mode without having to reach for the escape key, which can take a lot of time if you make hundreds of small edits.&lt;/p&gt;
&lt;p&gt;So I begin making changes, and then I write the buffer of the file onto disk with the ":w" vim command, which is connected to an autocmd hook in vim within ~/vim_runtime/my_configs.vim. This configuration calls the google chrome command line utility chrome-cli. Chrome-cli waits a little under a second for Pelican to build the static html pages, and then refreshes any tabs I have open that contain the title
of my blog,"deepython.com", in the description of the tab. &lt;/p&gt;
&lt;p&gt;Here is the specific autocmd added to ~/.vim_runtime/my_configs.vim file: &lt;/p&gt;
&lt;p&gt;autocmd BufWritePost *.md :!sleep .69 &amp;amp;&amp;amp; /Users/claytonblythe/github/version_control/scripts/reloadDeepython.sh&lt;/p&gt;
&lt;p&gt;The script reloadDeepython.sh contains the following: &lt;/p&gt;
&lt;p&gt;chrome-cli list tabs | grep deepython.com | cut -c 2-4 | while read -r line; do chrome-cli reload -t "$line"; done&lt;/p&gt;
&lt;p&gt;This assumes you have chrome-cli installed, which can be installed with "pip install chrome-cli"&lt;/p&gt;
&lt;p&gt;Using this configuration, I can get almost instant feed back about the aesthetics and content of my posts without even having to exit vim! &lt;/p&gt;
&lt;p&gt;It has been a pretty valuable way for me to learn about vim, python, and bash scripting, not to mention it is saving me a lot of time as well. &lt;/p&gt;
&lt;p&gt;Here is a photo of what my development environment looks like. Pretty slick huh? &lt;/p&gt;
&lt;p&gt;&lt;img alt="Alt Test" src="https://deepython.com/images/blogSetup.png"&gt; &lt;/p&gt;
&lt;p&gt;Here are some other links used in this post: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/amix/vimrc"&gt;vimrc&lt;/a&gt; github repo&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/claytonblythe/version_control/blob/master/scripts/reloadDeepython.sh"&gt;chrome-cli&lt;/a&gt; script&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/prasmussen/chrome-cli"&gt;chrome-cli&lt;/a&gt; github repo&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I hope that you find this useful. &lt;/p&gt;
&lt;p&gt;Until next time,&lt;/p&gt;
&lt;h4&gt;Clayton Blythe | &lt;em&gt;Deep Python&lt;/em&gt;&lt;/h4&gt;</content><category term="vim"></category></entry><entry><title>Manchester United Game Reminders with Python and Raspberry Pi</title><link href="/manchester-united-game-reminders-with-python-and-raspberry-pi.html" rel="alternate"></link><published>2017-09-09T13:10:00+00:00</published><updated>2017-09-09T13:10:00+00:00</updated><author><name></name></author><id>tag:None,2017-09-09:/manchester-united-game-reminders-with-python-and-raspberry-pi.html</id><summary type="html">&lt;p&gt;Suprisingly, I have found that I have a lot more free time now in the real world working than I did during undergraduate study, but that might be because I was doing things like research programs and internships on top of taking seventeen credits a semester. However, now that I …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Suprisingly, I have found that I have a lot more free time now in the real world working than I did during undergraduate study, but that might be because I was doing things like research programs and internships on top of taking seventeen credits a semester. However, now that I have this all this free time, I have started to follow sports more, specifically the English Premier League and NCAA Football. &lt;/p&gt;
&lt;p&gt;My favorite team in the English Premier League is Manchester United, however I couldn't be bothered to periodically check online, find their schedule, and determine when they play next. Additionally, I thought this would be a good opportunity for me to learn more about web scraping and python. So I built a simple web
scraping script to periodically parse Manchester United's upcoming schedule and text it to my iphone every couple of days. &lt;/p&gt;
&lt;p&gt;I have a python script called redDevilNotify.py running every two days on my Raspberry Pi which uses a Raspian Linux Distribution as the operating system. The Raspberry Pi is running 24/7 in my apartment, but it is very efficient and doesn't use much energy. 
The script uses the beautifulsoup python library to scrape the html of Manchester United's website for upcoming games, parses the html, converts the UTC dates to EST, and then it uses the Twilio API to text a reminder SMS to my iphone. &lt;/p&gt;
&lt;p&gt;The following crontab scheduling command is used to run the script, and you can edit this with the bash command bash:::crontab -e&lt;/p&gt;
&lt;p&gt;0 9 */2 * * /usr/bin/python3 /path/to/redDevilNotify.py &lt;/p&gt;
&lt;p&gt;This means that the script will run on the zeroth minute of the ninth hour every two days for every week and month of the year. &lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/claytonblythe/RedDevilNotify"&gt;Github Repo&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Until next time,&lt;/p&gt;
&lt;h4&gt;Clayton Blythe | &lt;em&gt;Deep Python&lt;/em&gt;&lt;/h4&gt;</content><category term="raspberry pi"></category><category term="python"></category><category term="web scraping"></category><category term="soccer"></category></entry><entry><title>Regex to the Rescue</title><link href="/regex-to-the-rescue.html" rel="alternate"></link><published>2017-09-08T15:10:00+00:00</published><updated>2017-09-08T15:10:00+00:00</updated><author><name></name></author><id>tag:None,2017-09-08:/regex-to-the-rescue.html</id><summary type="html">&lt;p&gt;For a project at work, I had to parse archaic mainframe flat files produced with Job Control Language (JCL) and COBOL, popular programming languages from decades ago. The difficulty is that for a given record or observation, there was no set way of distinguishing different fields, with no delimiters present …&lt;/p&gt;</summary><content type="html">&lt;p&gt;For a project at work, I had to parse archaic mainframe flat files produced with Job Control Language (JCL) and COBOL, popular programming languages from decades ago. The difficulty is that for a given record or observation, there was no set way of distinguishing different fields, with no delimiters present. The bytes were squished up next to each other, with a specific record structure determined by various COBOL Copy Books is what they are called. &lt;/p&gt;
&lt;p&gt;So the task was to generate a relational data structure out of these variable length records, and within each record there are various segments, each with different fields of different lengths in bytes. As you can see, this quickly got complicated. &lt;/p&gt;
&lt;p&gt;However we can bring regular expressions to the rescue!&lt;/p&gt;
&lt;p&gt;&lt;img alt="Alt Test" src="https://imgs.xkcd.com/comics/regular_expressions.png"&gt; &lt;/p&gt;
&lt;p&gt;(Source: https://imgs.xkcd.com/comics/regular_expressions.png)&lt;/p&gt;
&lt;p&gt;So I ended up looking for strings of text that start with a certain length of bytes for a key, followed by one of various segment indicators. Then I can use simple string splicing in Python with the shema specified by each segment's COBOL Copy Book to get the individual fields. &lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/claytonblythe/cobolRegex"&gt;Github Repo&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Until next time,&lt;/p&gt;
&lt;h4&gt;Clayton Blythe | &lt;em&gt;Deep Python&lt;/em&gt;&lt;/h4&gt;</content><category term="cobol"></category><category term="regex"></category><category term="python"></category></entry><entry><title>My Git Workflow</title><link href="/my-git-workflow.html" rel="alternate"></link><published>2017-09-02T13:10:00+00:00</published><updated>2017-09-02T13:10:00+00:00</updated><author><name></name></author><id>tag:None,2017-09-02:/my-git-workflow.html</id><summary type="html">&lt;h2&gt;Working Efficiently (a.k.a. Lazily)&lt;/h2&gt;
&lt;p&gt;I think it is very important to reduce the amount of typing that one has to do to keep track of projects, experiments, and working with version control, especially across multiple devices. As such, I have developed a sytem that I think works pretty …&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Working Efficiently (a.k.a. Lazily)&lt;/h2&gt;
&lt;p&gt;I think it is very important to reduce the amount of typing that one has to do to keep track of projects, experiments, and working with version control, especially across multiple devices. As such, I have developed a sytem that I think works pretty well, specifically for managing Github respositories and projects directly from the command line. Though it is heavily personalized to my own Github profile and there are other tools out there like Hub, I decided to build my own. &lt;/p&gt;
&lt;p&gt;First off, I made a bash script called newRepo.sh  which will create a new repository named after the first argument that I provide after the "newRepo" command. So if I type "newRepo pytorchTutorials", I will create a new github project called pytorchTutorials that resides in my Github repositories in the cloud. This allows me to quickly create new repos and projects from the command line without having to go to the Github website, create a repository, and then clone it through the command line. The newRepo script uses the Github API to create and
clone that newly made repository as well as add a predefined project directory structure for data, notebooks, figures, and scripts. &lt;/p&gt;
&lt;p&gt;The script is still a work in progress, and I am looking to make other scripts/aliases that will allow for creating private repositories and deleting them as well. &lt;/p&gt;
&lt;p&gt;Other useful git tools that I have made are aliases for cloning, commiting, and pushing changes to the remote repository. I have 'git status' as gs, 'git add --all' as gaa, and gcp "message here" as an alias to commit all the added changes with a provided commit message, and then push the changes. &lt;/p&gt;
&lt;p&gt;This tends to speed up my workflow greatly, and I hope you can find them useful! &lt;/p&gt;
&lt;p&gt;Most of the aliases can be found in my &lt;a href="https://github.com/claytonblythe/dotfiles"&gt;dotfiles repository&lt;/a&gt; and the newRepo script can be found in my &lt;a href="https://github.com/claytonblythe/version_control"&gt;version_control&lt;/a&gt; repository.&lt;/p&gt;
&lt;p&gt;Until next time,&lt;/p&gt;
&lt;h4&gt;Clayton Blythe | &lt;em&gt;Deep Python&lt;/em&gt;&lt;/h4&gt;</content><category term="git"></category><category term="version control"></category><category term="bash"></category></entry><entry><title>Spark MLlib Overview</title><link href="/spark-mllib-overview.html" rel="alternate"></link><published>2017-08-30T13:10:00+00:00</published><updated>2017-08-30T13:10:00+00:00</updated><author><name></name></author><id>tag:None,2017-08-30:/spark-mllib-overview.html</id><summary type="html">&lt;p&gt;This is an overview of the Spark MLlib framework, Spark's scalable machine learning library consisting of common machine learning algorithms and utilities that include tools for classification, regression, clustering, collaborative filtering, dimensionality reduction, as well as providing underlying basic summary statistics. It also contains various utilities for doing linear algrebra …&lt;/p&gt;</summary><content type="html">&lt;p&gt;This is an overview of the Spark MLlib framework, Spark's scalable machine learning library consisting of common machine learning algorithms and utilities that include tools for classification, regression, clustering, collaborative filtering, dimensionality reduction, as well as providing underlying basic summary statistics. It also contains various utilities for doing linear algrebra, statistics, and general handling of data. MLlib uses the linear algebra package called Breeze, which depends on the netlib-java for optimized numerical processing. Spark MLlib is distinct from Spark ML, as it deals with RDD (Resilient Distributed Datasets) instead of DataFrames. RDD's are fundamental data structures of Spark, which are divided into logical partitions and distributed across different nodes of a cluster. Spark makes use of the concept of RDD to achieve faster and more efficient MapReduce operations. It is also fault tolerant as well as in-memory data processing, which is 10 to 100 times faster than network and Disk.&lt;/p&gt;
&lt;p&gt;Here is a broad overview of the capabilities of Spark MLlib:&lt;/p&gt;
&lt;h2&gt;Basic Statistics&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Correlation&lt;/em&gt; computes the correlation matrix for the input Dataset of vectors, and the output will be a DataFrame that contains the correlation matrix of the column of vectors.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Hypothesis Testing&lt;/em&gt; is possible thorugh a ChiSquare test to conduct a Pearson independence test for every feature against the label or target. For each feature, the feature label pairs are converted into a contingency matrix for which the Chi-squared statistic is computed.&lt;/p&gt;
&lt;h2&gt;ML Pipelines&lt;/h2&gt;
&lt;p&gt;Inspired by the scikit-learn project, MLlib standardizes APIs for machine learning algorithms to make it easier to combine multiple algorithms into a single pipeline or workflow.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;DataFrame&lt;/em&gt; ML API uses DataFrame form Spark SQL as an ML dataset, which can hold a variety of data types.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Transformer&lt;/em&gt; is an algorithm that can transform one DataFrame into another DataFrame (e.g. an ML model is a Transformer which transforms a DataFrame into a DataFrame with predictions).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Estimator&lt;/em&gt; is an algorithm which can be fit on a DataFrame to produce a Transformer (this would be a learning algorithm which produces a model)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Pipeline&lt;/em&gt; chains multiple Transformers and Estimators together for a ML workflow.&lt;/p&gt;
&lt;h2&gt;Feature Selection &amp;amp; Transformation&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;TF-IDF&lt;/em&gt; (Term frequency-inverse document frequency) is a feature vectorization method used in text mining to reflec thte importance of a term to a document in the corpus.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Word2Vec&lt;/em&gt; is an Estimator which takes sequences of words representing documents and trains a Word2VecModel, which maps each word to a unique fixed-size vector.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;CountVectorizer&lt;/em&gt; aim to convert a collection of text documents to vectors of token counts.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Tokenizer&lt;/em&gt; is a simple class providing functionality for taking text and breaking it into individual terms (usually words), there is also RegexTokenizer which allows more advanced tokenization based on regex matching.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;StopWordsRemover&lt;/em&gt; takes an input of a sequence of strings and drops all the stop words from the input sequences.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;NGram&lt;/em&gt; takes an input of a sequence of strings and the parameter n is used to determine the number of terms in each n-gram. The output will be a sequence of n-grams where each n-gram is represented by a space-delimited string of n consecutive words.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;PCA&lt;/em&gt; is a statistical procedure that uses orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Polynomial Expansion&lt;/em&gt; is the process of expanding your features into a polynomial space, which is formulated by an n-degree combination of original dimensions.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;OneHotEncoder&lt;/em&gt; allows for mapping a column of label indices to a column of binary vectors. This enables algorithms such as logistic regression to utilize categorical variables.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;VectorIndexer&lt;/em&gt; helps index categorical features in datasets of Vectors. It can allow algorithms such as Decision Trees and Tree Ensembles to treat categorical features appropriately, improving performance.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Interaction&lt;/em&gt; is a Transformer which takes vector or double-valued columns and can generate their interactions.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Normalizer&lt;/em&gt; is a Transformer which transforms a dataset of Vector rows, normalizing each Vector to have unit norm. It takes some parameter p, which uses the p-norm used for normalization.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;StandardScaler&lt;/em&gt; transforms a dataset of Vector rows, normalizing each feature to have unit standard deviation and/or zero mean.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;MinMaxScaler&lt;/em&gt; is often used to rescale a feature to a specific range like [0,1].&lt;/p&gt;
&lt;p&gt;&lt;em&gt;MaxAbsScaler&lt;/em&gt; transforms a dataset of Vector rows to a range of [-1,1].&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Bucketizer&lt;/em&gt; transforms a column of continous features to a column of feature buckets.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Imputer&lt;/em&gt; is a transformer that completes missing values in a dataset, either using the mean or the median.&lt;/p&gt;
&lt;h2&gt;Feature Selectors&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;VectorSlicer&lt;/em&gt; is a transformer that takes a feature vecotr and outputs a new feature vector with a sub-array of the original features. It is useful for extracting features from a vector column.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;RFormula&lt;/em&gt; selects columns specified by an R model formula, (~, ., +, etc.)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;ChiSqSelector&lt;/em&gt; uses Chi-Squared tests of independence to decide which features to use, from a fixed number of top features.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;LSH Algorithms&lt;/em&gt; like Bucketed Random Projection for Euclidian distance and MinHash for Jaccard Distance.&lt;/p&gt;
&lt;h2&gt;Classification and Regression&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Logistic Regression&lt;/em&gt; is supported with summary statistics, as well as multinomial logistic regression.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;DecisionTreeClassifier&lt;/em&gt; is a tree based calssification and regression model and so is &lt;em&gt;RandomForestClassificationModel&lt;/em&gt; and &lt;em&gt;GBTClassificationModel&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;MultilayerPerceptronClassifier&lt;/em&gt; is a classifier based on feedforward artificial neural networks, and it consists of fully connected layers that itulize a simoid logistic function and the output layer uses a softmax function. The number of nodes in the output layer corresponds to the number of classes to be classified.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;LinearSVC&lt;/em&gt; is a support vector machine that represents a hyperplane or set of hyperplanes in a high or infinite dimensional space that can be used for classification, regression, or other tasks.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;NaiveBayes&lt;/em&gt; allows for simple probabilistic classifiers on applying Bayes' theorem with strong (naive) independence assumptions between the features.&lt;/p&gt;
&lt;p&gt;For regression problems, &lt;em&gt;LinearRegression&lt;/em&gt; and &lt;em&gt;GeneralizedLinearRegression&lt;/em&gt; are supported.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;AFTSurvivalRegression&lt;/em&gt; employs an Accelerated failure time (AFT) model, which is a parametric survival regression model for censored data. It is a log-linear model for survival analysis and is easier to parallelize.&lt;/p&gt;
&lt;p&gt;Other methods like ensembles of decision trees, random forest, and gradient-boosted trees are supported.&lt;/p&gt;
&lt;h2&gt;Clustering&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;K-Means&lt;/em&gt; is a popular used clustering algorithm that clusters the data points into a predefined number of clusters. The MLlib implementation includes a parallelized variant of the k-means++ method.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Latent Dirichlet Allocation&lt;/em&gt;, &lt;em&gt;Bisecting k-means&lt;/em&gt;, and &lt;em&gt;Gaussian Mixture Model (GMM)&lt;/em&gt; are all supported as well.&lt;/p&gt;
&lt;h2&gt;Collaborative Filtering&lt;/h2&gt;
&lt;p&gt;Spark MLlib supports collaborative filtering, which are techniques that aim to filll in the missing entries in a user-item association matrix. It currently supports model-based collaborative filtering, in which users and products are described by a small set of latent factors that can be used to predict missing entries. Spark.ml uses the alternating least squares algorithm to learn these latent factors.&lt;/p&gt;
&lt;h2&gt;Frequent Pattern Mining&lt;/h2&gt;
&lt;p&gt;Spark MLlib has capabilities for mining frequent items, itemsets, subsequences, or other substructures that are cmomonly the first steps for analyzing a large-scale dataset. Given a dataset of transactions, the first step is to calculate item frequencies and identify frequent items. The second step uses a suffix tree structure to encode transactions without generating candidate sets explicitly.&lt;/p&gt;
&lt;h2&gt;Model Selection&lt;/h2&gt;
&lt;p&gt;MLlib supports model selection using tools like &lt;em&gt;CrossValidator&lt;/em&gt; and &lt;em&gt;TrainValidationSplit&lt;/em&gt; which provide useful tools for hyperparameter tuning and parameter grids to search over.&lt;/p&gt;
&lt;p&gt;Until next time,&lt;/p&gt;
&lt;h4&gt;Clayton Blythe | &lt;em&gt;Deep Python&lt;/em&gt;&lt;/h4&gt;</content><category term="spark"></category></entry><entry><title>Personalized Bash Environment</title><link href="/personalized-bash-environment.html" rel="alternate"></link><published>2017-08-18T22:34:00+00:00</published><updated>2017-08-18T22:34:00+00:00</updated><author><name></name></author><id>tag:None,2017-08-18:/personalized-bash-environment.html</id><summary type="html">&lt;h2&gt;Understanding Bash Startup Files&lt;/h2&gt;
&lt;p&gt;In order to do efficient work in a unix/linux environment, it is important to get a personalized environment setup to use various aliases, navigate to default directories, and to get your proper path environment setup. This article is meant to summarize the difference between .bashrc …&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Understanding Bash Startup Files&lt;/h2&gt;
&lt;p&gt;In order to do efficient work in a unix/linux environment, it is important to get a personalized environment setup to use various aliases, navigate to default directories, and to get your proper path environment setup. This article is meant to summarize the difference between .bashrc, .bash_profile, and .profile files and what I think the best setup is regarding these files. It is standard for these files to be stored in the home directory '~', which is the  &lt;a href="https://www.gnu.org/software/bash/manual/bashref.html#Tilde-Expansion"&gt;Tilde Expansion&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;It's helpful to think about how Bash chronologically behaves when it is invoked as a interactive login session. When Bash is invoked as interactive, it first reads from the file /etc/profile if that file exists. After reading that file, it will look for a file at ~/.bash_profile, ~/.bash_login, and ~/.profile in that order. It will execute commands from the first one that is available if it exists, and it will not proceed unless explicitly told so. &lt;/p&gt;
&lt;p&gt;If Bash is invoked as an interactive non-login session, then Bash reads and executes commands from ~/.bashrc if that file exists. Therefore, it is common to include the following commands in ~/.bash_profile, as it is often desired to ensure that ~/.bashrc is executed regardless of whether Bash is executed in a login or non-login session.  &lt;/p&gt;
&lt;p&gt;if [ -f ~/.bashrc ]; then
   source ~/.bashrc
fi&lt;/p&gt;
&lt;p&gt;So here in ~/.bashrc you can put all various starting configurations like your environment's PATH, and you can source a file containing aliases, like mine is at ~/.aliases. So every time you have a terminal session, whether it is a login session or simply another window, you can have your personalized environment ready for you. &lt;/p&gt;
&lt;p&gt;Just to specify, for remote shell logins, like a ssh login, Bash will read /etc/profile and then one of ~/.bash_profile or ~/.profile. &lt;/p&gt;
&lt;p&gt;For more reading, you can visit the bash manual and user guide &lt;a href="https://www.gnu.org/software/bash/manual/bashref.html#Introduction"&gt;here&lt;/a&gt;, or you can see another helpful guide &lt;a href="https://mywiki.wooledge.org/DotFiles"&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I think my setup will most likely include a sourcing of aliases from ~/.aliases within my ~/.bashrc file, which is sourced for every terminal session, either login or non-login sessions. This seems to be a good setup for keeping things tidy and organized. I am open to suggestions and advice if anyone reads this post!&lt;/p&gt;
&lt;p&gt;Until next time,&lt;/p&gt;
&lt;h4&gt;Clayton Blythe | &lt;em&gt;Deep Python&lt;/em&gt;&lt;/h4&gt;</content><category term="bash"></category><category term="version control"></category></entry><entry><title>Introducing Deep Python</title><link href="/introducing-deep-python.html" rel="alternate"></link><published>2017-08-13T15:01:00+00:00</published><updated>2017-08-13T15:01:00+00:00</updated><author><name></name></author><id>tag:None,2017-08-13:/introducing-deep-python.html</id><summary type="html">&lt;p&gt;Python(noun) : &lt;em&gt;A large heavy-bodied nonvenomous constrictor snake occurring throughout the Old World tropics.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;...Sorry to get your hopes up, but this won't be a discussion about snakes nor the reptilian overlords that run the U.S government.&lt;/p&gt;
&lt;p&gt;Instead, this is the first post of my new blog called &lt;em&gt;Deep …&lt;/em&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;Python(noun) : &lt;em&gt;A large heavy-bodied nonvenomous constrictor snake occurring throughout the Old World tropics.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;...Sorry to get your hopes up, but this won't be a discussion about snakes nor the reptilian overlords that run the U.S government.&lt;/p&gt;
&lt;p&gt;Instead, this is the first post of my new blog called &lt;em&gt;Deep Python&lt;/em&gt; @ &lt;a href="http://deepython.com"&gt;deepython.com&lt;/a&gt;. I am writing this blog as a hobby, as I begin my own personal learning adventure and career in the domain of neural networks, deep learning, and
artificial intelligence. I plan to use this as an outreach avenue, but also as a knowledge repository for myself and others too.&lt;/p&gt;
&lt;p&gt;Just so you know what's to come, the blog will cover a wide range of topics that describe projects that I take on for fun, and I might even write about some miscellaneous topics as well. I imagine that I will cover relevant topics like &lt;a href="https://www.wikiwand.com/en/Python_programming_language"&gt;Python&lt;/a&gt;, &lt;a href="https://www.wikiwand.com/en/Torch_machine_learning"&gt;Pytorch&lt;/a&gt;, &lt;a href="https://www.wikiwand.com/en/TensorFlow"&gt;TensorFlow&lt;/a&gt;, &lt;a href="https://www.wikiwand.com/en/Linux"&gt;Linux&lt;/a&gt;, &lt;a href="https://www.google.com/search?q=pyspark&amp;amp;oq=pyspark&amp;amp;aqs=chrome..69i57j0l5.750j0j4&amp;amp;sourceid=chrome&amp;amp;ie=UTF-8"&gt;Spark&lt;/a&gt;, &lt;a href="https://www.wikiwand.com/en/Git"&gt;Git&lt;/a&gt;, &lt;a href="http://pokemon.wikia.com/wiki/Vulpix"&gt;Vulpix&lt;/a&gt; and &lt;a href="https://www.wikiwand.com/en/Docker_software"&gt;Docker&lt;/a&gt;... Now which one of those was a pokemon? &lt;/p&gt;
&lt;p&gt;Joking aside, don't be surprised at an occasional excursion into economics, current news, or personal finance.
Anyways, I think that is long enough for my first post. I'll keep it short and sweet, but be sure that I have quite a few ideas for posts to come!&lt;/p&gt;
&lt;p&gt;Feel free to send me an email at &lt;a href="mailto:claytondblythe@gmail.com"&gt;claytondblythe@gmail.com&lt;/a&gt; if you have ideas for projects, posts, or suggestions for improvement.&lt;/p&gt;
&lt;p&gt;Until next time,&lt;/p&gt;
&lt;h4&gt;Clayton Blythe | &lt;em&gt;Deep Python&lt;/em&gt;&lt;/h4&gt;</content><category term="introduction"></category><category term="python"></category></entry></feed>