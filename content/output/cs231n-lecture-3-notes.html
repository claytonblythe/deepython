<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
        <title>CS231n Lecture 3 Notes</title>
        <link rel="stylesheet" href="/theme/css/main.css" />
        <link href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="A Pelican Blog Atom Feed" />

        <!--[if IE]>
            <script src="https://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
        <![endif]-->
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="/">A Pelican Blog </a></h1>
                <nav><ul>
                    <li class="active"><a href="/category/deep-learning.html">Deep Learning</a></li>
                    <li><a href="/category/docker.html">Docker</a></li>
                    <li><a href="/category/misc.html">Misc</a></li>
                    <li><a href="/category/other.html">Other</a></li>
                </ul></nav>
        </header><!-- /#banner -->
<section id="content" class="body">
  <article>
    <header>
      <h1 class="entry-title">
        <a href="/cs231n-lecture-3-notes.html" rel="bookmark"
           title="Permalink to CS231n Lecture 3 Notes">CS231n Lecture 3 Notes</a></h1>
    </header>

    <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2017-08-29T13:10:00+00:00">
                Published: Tue 29 August 2017
        </abbr>

<p>In <a href="/category/deep-learning.html">Deep Learning</a>.</p>
<p>tags: <a href="/tag/cs231n.html">cs231n</a> <a href="/tag/notes.html">notes</a> <a href="/tag/python.html">python</a> <a href="/tag/neural-networks.html">neural networks</a> </p>
</footer><!-- /.post-info -->      <p>I have recently been making my way through the Stanford Master's in Computer Science 231n course. This particular lecture
was about defining a loss function for how a simple Linear Classifier performs at classifying categories during training time,
and this metric in a sense quantifies how "unhappy" our scores are across the training data.</p>
<p>The task then is to find a way to efficiently find parameters (Weights and Biases) that minimize and optimize the loss function, useually via some optimization algorithm like Gradient Descent.</p>
<h3>Loss Functions</h3>
<ol>
<li>Multiclass SVM Loss</li>
<li>Softmax Classifier (Cross-Entropy Loss / Multinomial Logistic Regression)</li>
</ol>
<h3><em>Multiclass SVM Loss</em></h3>
<p>The average across all the differences of the scores between the correct class and incorrect classes with a constant of one added. 1/N * Sigma { ((Incorrect class score  - Correct class score) + 1)} . Using a value of one here is
arbitrary and really just determines what magnitude the weights can be.</p>
<p>Here when we initialize the weights, they are chosen to be small numbers, so in this case the initial value for the loss will be 2. Weight values can be multiplied in the same way, and they could be twice as large and achieve the same Loss (ignoring bias).</p>
<h3><em>Softmax Loss</em></h3>
<p>Softmax Loss is a different functional form for how loss is specified across scores. This assumes that the scores are unormalized log probabilities for each class. To get probabilities for each class,
we take the exponentiated scores for each element divided by the sum of all exponentiated elements. So here we want to maximize the log likelihood, or for a loss function we want to
minimize the negative log likelihood of the correct class. It turns out that maximizing this is more mathematically conducive than maximizing the negative probabilities themselves.
For the example of classifying a cat, if the normalized probability of a cat class is .13 then the loss would be -log(.13)= .89, and we are trying to maximize this, where zero is the minimum and there is no bounded maximum.</p>
<p>When we initialize weights we typically choose them to be very small, so there should be an initial loss of -log( 1 / number of classes), as the initial scores would be zero, then unormalized probabilities of 1 for every class, then
the loss should be -log( 1 / # of classes ). As the model trains, the loss should move toward zero.</p>
<p>Optimization occurs by finding the gradient of the loss function with respect to certain parameters, usually the weights for each class. In practice an analytic gradient is used, which is an exact, fast, but error-prone method.
You often then do a gradient check, where you compare the numerical gradient which is usually approximate, slow, but easy to write compared to your analytic gradient.</p>
<h3><em>Weight Regularization</em></h3>
<p>Weight regularization is a set of techniques to add objectives to the loss function, such that there exists a tradeoff between training error and generalization error. 
The most common form of regularization in neural networks is L2 regularization, also known as weight decay. This push$
Therefore regularization loss is a new component that contributes to the overall loss, and it is only a function of 
Including this weight regularization in the overall loss function that you are trying to minimize leads to weights that are diffuse, making sure that the network does not overfit certain regions of the image. This leads to better generalization performance at testing time.</p>
<h3><em>Stochastic Gradient Descent</em></h3>
<p>This process is usually composed of two steps:
  1. Find the weights gradient by evaluating the gradient of the loss function with respect to the parameters of your training data, the weights. 
  2. Set new weights by multiplying step size (a.k.a. learning rate) by the gradient of the loss function with respect to weights, most importantly in the direction of the negative gradient. The gradient points in the direction of maximal increase, so the negative gradient will modify the parameters of the network closer to minimizing the loss function, or at least moving toward some local minimum.</p>
<p>The learning rate/step size is an important hyperparameter for this.</p>
<h3><em>Mini-batch Gradient Descent</em></h3>
<p>Instead of using all training samples for each iteration (finding the gradient of the loss function corresponding to all your training data), you can use a small <em>batch</em> comprised of a small subset of your training data. Then you can get a good approximation of the gradient and use smaller step sizes rather than using a full-batch size for each iteration or epoch.
Often this isn't a very significant hyperparameter to tune, but rather you choose this based on your GPU architecture and the constraints of your memory.
The key is finding the appropriate learning rate to converge over time across epochs (full cycles through your training data).</p>
<p>The loss function can be thought of as an optimization problem in high-dimensional space, in which we are trying to reach the bottom of some high-dimensional valley. We start with some random initialization of weights and through iterative differentiation and adjustment we can reach the bottom. The next important concept to cover will be backprogation, essentially how to compute the gradient analytically by using the chain rule. </p>
<p>The advancements recently (since roughly 2012) of using these techniques for neural networks are that you do not have to hand-craft features regarding your images, but rather you can train your entire network and the network automatically learns feature
without explicitly being programmed the structure of features or objects, like most rule-based recognition systems of the past were. Here the networks can be trained all the way back to the raw pixels, which make them very powerful and flexible at solving a wide array of problems in sound, image, and pattern recognition.</p>
<p>Until next time,</p>
<h4>Clayton Blythe | <em>Deep Python</em></h4>
    </div><!-- /.entry-content -->

  </article>
</section>
        <section id="extras" class="body">
                <div class="social">
                        <h2>social</h2>
                        <ul>
                            <li><a href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate">atom feed</a></li>

                        </ul>
                </div><!-- /.social -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a href="http://getpelican.com/">Pelican</a>, which takes great advantage of <a href="http://python.org">Python</a>.
                </address><!-- /#about -->

                <p>The theme is by <a href="http://coding.smashingmagazine.com/2009/08/04/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
        </footer><!-- /#contentinfo -->

</body>
</html>