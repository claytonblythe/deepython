<!DOCTYPE html>
<html lang="en-US">
    <head>
        <meta charset="utf-8"> 
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="author" content="Clayton Blythe" />
        <meta name="copyright" content="Clayton Blythe" />

        <meta property="og:type" content="article" />
        <meta name="twitter:card" content="summary">

<meta name="keywords" content="cs231n, notes, python, neural networks, CS231n, " />

<meta property="og:title" content="CS231n Lecture 3 Notes "/>
<meta property="og:url" content="http://localhost:8000/cs231n-lecture-3-notes.html" />
<meta property="og:description" content="I have recently been making my way through the Stanford Computer Science 231n course. This particular lecture was about defining a loss function for how the Linear Classifier performs across all of the training data, and this in a sense quantifies how &#34;unhappy&#34; our scores are across the training data …" />
<meta property="og:site_name" content="deepython.com | Clayton Blythe" />
<meta property="og:article:author" content="Clayton Blythe" />
<meta property="og:article:published_time" content="2017-08-29T13:10:00-05:00" />
<meta name="twitter:title" content="CS231n Lecture 3 Notes ">
<meta name="twitter:description" content="I have recently been making my way through the Stanford Computer Science 231n course. This particular lecture was about defining a loss function for how the Linear Classifier performs across all of the training data, and this in a sense quantifies how &#34;unhappy&#34; our scores are across the training data …">

        <title>CS231n Lecture 3 Notes  · deepython.com | Clayton Blythe
</title>
        <link href="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/css/bootstrap-combined.min.css" rel="stylesheet">
        <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.1/css/font-awesome.css" rel="stylesheet">
        <link rel="stylesheet" type="text/css" href="http://localhost:8000/theme/css/pygments.css" media="screen">
        <link rel="stylesheet" type="text/css" href="http://localhost:8000/theme/tipuesearch/tipuesearch.css" media="screen">
        <link rel="stylesheet" type="text/css" href="http://localhost:8000/theme/css/elegant.css" media="screen">
        <link rel="stylesheet" type="text/css" href="http://localhost:8000/theme/css/custom.css" media="screen">
        <link rel="shortcut icon" href="http://localhost:8000/theme/images/favicon.ico" type="image/x-icon" type="image/png" />
        <link rel="icon" href="http://localhost:8000/theme/images/apple-touch-icon-152x152.png" type="image/png" />
        <link rel="apple-touch-icon" href="http://localhost:8000/theme/images/apple-touch-icon.png"  type="image/png" />
        <link rel="apple-touch-icon" sizes="57x57" href="http://localhost:8000/theme/images/apple-touch-icon-57x57.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="72x72" href="http://localhost:8000/theme/images/apple-touch-icon-72x72.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="76x76" href="http://localhost:8000/theme/images/apple-touch-icon-76x76.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="114x114" href="http://localhost:8000/theme/images/apple-touch-icon-114x114.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="120x120" href="http://localhost:8000/theme/images/apple-touch-icon-120x120.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="144x144" href="http://localhost:8000/theme/images/apple-touch-icon-144x144.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="152x152" href="http://localhost:8000/theme/images/apple-touch-icon-152x152.png" type="image/png" />
    </head>
    <body>
        <div id="content-sans-footer">
        <div class="navbar navbar-static-top">
            <div class="navbar-inner">
                <div class="container-fluid">
                    <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </a>
                    <a class="brand" href="http://localhost:8000/"><span class=site-name> <span style="color:#254601;">deepython.com | Clayton Blythe </span></span></a>
                    <div class="nav-collapse collapse">
                        <ul class="nav pull-right top-menu">
                            <li ><a href="http://localhost:8000">Home</a></li>
                            <li ><a href="http://localhost:8000/categories.html">Categories</a></li>
                            <li ><a href="http://localhost:8000/tags.html">Tags</a></li>
                            <li ><a href="http://localhost:8000/archives.html">Archives</a></li>
                            <li><form class="navbar-search" action="http://localhost:8000/search.html" onsubmit="return validateForm(this.elements['q'].value);"> <input type="text" class="search-query" placeholder="Search" name="q" id="tipue_search_input"></form></li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
        <div class="container-fluid">
            <div class="row-fluid">
                <div class="span1"></div>
                <div class="span10">
<article>
<div class="row-fluid">
    <header class="page-header span10 offset2">
    <h1><a href="http://localhost:8000/cs231n-lecture-3-notes.html"> CS231n Lecture 3 Notes  </a></h1>
    </header>
</div>

<div class="row-fluid">
        <div class="span8 offset2 article-content">

            
            <p>I have recently been making my way through the Stanford Computer Science 231n course. This particular lecture
was about defining a loss function for how the Linear Classifier performs across all of the training data,
and this in a sense quantifies how "unhappy" our scores are across the training data.</p>
<p>The task then is to find a way to efficiently find the parameters (Weights and Biases) that minimize and optimize the loss function via some optimization algorithm like Gradient Descent.</p>
<h3>Loss Functions</h3>
<ol>
<li>Multiclass SVM Loss</li>
<li>Softmax Classifier: (Cross-Entropy Loss / Multinomial Logistic Regression)</li>
</ol>
<h3><em>Multiclass SVM Loss</em>:</h3>
<p>The average across all the differences of the scores between the correct class and incorrect classes with a constant of one added. 1/N * Sigma { ((Incorrect class score  - Correct class score) + 1)} . Using a value of one here is
arbitrary and really just determines what magnitude the weights can be.</p>
<p>Here when we initialize the weights, they are chosen to be small numbers, so in this case the initial value for the loss will be 2. W values can be multiplied and they could be twice as large and achieve the same Loss (ignoring bias).</p>
<h3><em>Weight Regularization</em>:</h3>
<p>A set of techniques to add objectives to the loss, to tradeoff between training error and generalization error. i.e. a trade off between bias and variance 
The most common form of regularization is L2 regularization, also known as weight decay. This pushes the weights to being smaller and more diffused across all pixels and features, which prevents overfitting, leading to better generalization performance.
Therefore regularization loss is a new component that contributes to the overall loss, and is only a function of the weights, not your dataset.
Including this weight regularization in the loss function that you are trying to minimize leads to weights that not only classify correctly but prefer diffuse weights, taking into account as much of the image as possible.</p>
<p>Why would we want more evenly spread weights? This allows for taking in more parts and features of the input image into account.</p>
<h3><em>Softmax Loss</em>:</h3>
<p>Softmax Loss is a different functional form for how loss is specified across scores. This assumes that the scores are unormalized log probabilities for each class. To get probabilities for each class,
we take the exponentiated scores for each element divided by the sum of all exponentiated elements. So here we want to maximize the log likelihood, or for a loss function we want to
minimize the negative log likelihood of the correct class. It turns out that maximizing this is more mathematically conducive than maximizing the negative probabilities themselves.
For the example of classifying a cat, if the normalized probability of a cat class is .13 then the loss would be -log(.13)= .89, and we are trying to maximize this, where zero is the minimum and there is no bounded maximum.</p>
<p>When we initialize weights we typically choose them to be very small, so there should be an initial loss of -log( 1 / number of classes), as the initial scores would be zero, then unormalized probabilities of 1 for every class, then
the loss should be -log( 1 / # of classes ). As the model trains, the loss should move toward zero.</p>
<p>Optimization occurs by finding the gradient of the loss function with respect to certain parameters, usually the weights for each class. In practice an analytic gradient is used, which is an exact, fast, but error-prone method.
You often then do a gradient check, where you compare the numerical gradient which is usually approximate, slow, but easy to write compared to your analytic gradient.</p>
<h3><em>Stochastic Gradient Descent</em>:</h3>
<p>This process is usually composed of two steps:
  1. Find the weights gradient by evaluating the gradient of the loss function with respect to data, weights
  2. Set new weights by multiplying step size (a.k.a. learning rate) by the gradient of the loss function with respect to weights, in the direction of the negative gradient. The gradient points in the direction of maximal increase, so the negative gradient will lead to minimizing the loss function, or at least moving toward some local or global minimum.</p>
<p>Your learning rate / step size are an important hyperparameter for this.</p>
<h3><em>Mini-batch Gradient Descent</em>:</h3>
<p>Instead of using all training samples for each iteration (finding the gradient of the loss function corresponding to all your training data), you can use a small <em>batch</em> size comprised of a small subset of your training data. Then you can get a good approximation of the gradient and use smaller step sizes rather than using a full-batch size for each iteration or epoch.
Often this isn't a very significant hyperparameter to tune, but rather you choose this based on your GPU architecture and the constraints of your memory.
The key is finding the appropriate learning rate to converge over time across epochs (iterations).</p>
<p>The loss function can be thought of as an optimization problem in high-dimensional space, in which we are trying to reach the bottom of some high-dimensional valley. We start with some random initialization of weights and through iterative differentiation and adjustement we can reach the bottom. The next important concept to cover will be backprogation, essentially how to compute the gradient analytically using the chain rule. </p>
<p>The advancements recently (since roughly 2012) of using these techniques for neural networks are that you do not have to hand-craft features regarding your images, but rather you can train your entire network and feature
extraction without explicitly programming the structure of different features or objects into some rule-based recognition system. Here the networks can be trained all the way back to the raw pixels, which make them very powerful and flexible.</p>
<p>Until next time,</p>
<h4>Clayton Blythe | <em>Deep Python</em></h4>
            
            
            <hr/>
        </div>
        <section>
        <div class="span2" style="float:right;font-size:0.9em;">
            <h4>Published</h4>
            <time pubdate="pubdate" datetime="2017-08-29T13:10:00-05:00">Aug 29, 2017</time>
            <h4>Category</h4>
            <a class="category-link" href="http://localhost:8000/categories.html#cs231n-ref">CS231n</a>
            <h4>Tags</h4>
            <ul class="list-of-tags tags-in-article">
                <li><a href="http://localhost:8000/tags.html#cs231n-ref">cs231n
                    <span>2</span>
</a></li>
                <li><a href="http://localhost:8000/tags.html#neural-networks-ref">neural networks
                    <span>3</span>
</a></li>
                <li><a href="http://localhost:8000/tags.html#notes-ref">notes
                    <span>2</span>
</a></li>
                <li><a href="http://localhost:8000/tags.html#python-ref">python
                    <span>5</span>
</a></li>
            </ul>
<h4>Contact</h4>
    <a href="http://linkedin.com/in/claytonblythe" title="My LinkedIn Profile" class="sidebar-social-links" target="_blank">
    <i class="fa fa-linkedin sidebar-social-links"></i></a>
    <a href="http://github.com/claytonblythe" title="My Github Profile" class="sidebar-social-links" target="_blank">
    <i class="fa fa-github sidebar-social-links"></i></a>
        </div>
        </section>
</div>
</article>
                </div>
                <div class="span1"></div>
            </div>
        </div>
        <div id="push"></div>
    </div>
<footer>
<div id="footer">
    <ul class="footer-content">
        <li class="elegant-license"><span xmlns:dct="http://purl.org/dc/terms/" property="dct:title">deepython.com</span> by <a xmlns:cc="http://creativecommons.org/ns#" href="http://deepython.com" property="cc:attributionName" rel="cc:attributionURL">Clayton Blythe</a> is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/3.0/deed.en_US">Creative Commons Attribution-ShareAlike 3.0 Unported License</a>.</li>
        <li class="elegant-power">Powered by <a href="http://getpelican.com/" title="Pelican Home Page">Pelican</a>. Theme: <a href="http://oncrashreboot.com/pelican-elegant" title="Theme Elegant Home Page">Elegant</a> by <a href="http://oncrashreboot.com" title="Talha Mansoor Home Page">Talha Mansoor</a></li>
    </ul>
</div>
</footer>            <script src="http://code.jquery.com/jquery.min.js"></script>
        <script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/js/bootstrap.min.js"></script>
        <script>
            function validateForm(query)
            {
                return (query.length > 0);
            }
        </script>

    
    </body>
    <!-- Theme: Elegant built for Pelican
    License : http://oncrashreboot.com/pelican-elegant -->
</html>